{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e36639-29b5-49a1-bd97-3e8fa27573d3",
   "metadata": {},
   "source": [
    "# Spark (Structured) Streaming\n",
    "\n",
    "This is an illustative example for the usage of spark for streaming. We are doing something similar to the assignment, i.e., frequency counting, but now making use of the highly scalable and robust streaming library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de194b9-31b9-456b-a6f0-905ef28766c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T14:51:56.329857100Z",
     "start_time": "2024-01-17T14:51:51.536727700Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# use the following if you are running this notebook on your host machine and started the spark-connect service on the spark-master container\n",
    "# spark = SparkSession.builder.remote('sc://localhost:15002').appName('streaming-exercise').getOrCreate()\n",
    "# use this line instead if you are running the notebook on the spark-master container itself\n",
    "spark = SparkSession.builder.master('spark://spark-master:7077').appName('streaming-exercise').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea65cd6-146a-4938-b944-0572364cddb7",
   "metadata": {},
   "source": [
    "### Stream Query Setup\n",
    "Spark Structured Streaming uses \"watermarking\" to make exactly-once guarantees for their data handling.\n",
    "This is out-of-scope for us to treat in-depth but, essentially, this sets a maximal delay after which incoming but delayed data will be counted. (this is relevant if events have an _event creation_ time that can differ from their _processing time_, i.e., due to network delays)\n",
    "The purpose here is also to be able to delete old state information that is older than this delay.\n",
    "\n",
    "After this \"watermarking\" step, you can see the application of stateful and stateless operators, such as _filter_, to the stream.\n",
    "In the end, two different _sinks_ are defined.\n",
    "\n",
    "As an exercise, try to draw the computation graph (like on slide 90 in the Big Data lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96532a-eef3-47b2-9f67-600e3e7b9043",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T14:52:17.196893400Z",
     "start_time": "2024-01-17T14:52:14.073885900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format('socket') \\\n",
    "    .option('host', 'localhost') \\\n",
    "    .option('port', 9999) \\\n",
    "    .load()\n",
    "\n",
    "# extract words\n",
    "words = lines.select(\n",
    "   F.explode(\n",
    "       F.split(lines.value, ' ')\n",
    "   ).alias('word')\n",
    ")\n",
    "\n",
    "# watermark\n",
    "words_with_time = words.withColumn('timestamp', F.current_timestamp()).withWatermark('timestamp', '5 seconds')\n",
    "\n",
    "# F.window creates sliding/moving windows\n",
    "catpm = words_with_time.groupby(F.window('timestamp', '60 seconds', '5 seconds'), 'word').agg(F.count('word').alias('cats per minute')).drop('word')\n",
    "\n",
    "word_counts = words_with_time.groupBy(F.window('timestamp', '60 seconds', '30 seconds'), 'word').count()\n",
    "\n",
    "catpm_output = catpm.writeStream.outputMode('complete').format('memory').queryName('catpm')\n",
    "word_counts_output = word_counts \\\n",
    "    .writeStream \\\n",
    "    .outputMode('complete') \\\n",
    "    .format('memory') \\\n",
    "    .queryName('wordcounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436456c5-09f3-431e-9a35-3d1d42b1d2d9",
   "metadata": {},
   "source": [
    "Before you start the queries, you have to open the socket via the ´nc´ commmand.\n",
    "\n",
    "e.g. ´docker exec -it spark-master nc -lk 9999´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b2330-dd62-4cf6-9223-a5ac0e0a21ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T14:52:50.600500800Z",
     "start_time": "2024-01-17T14:52:43.289390Z"
    }
   },
   "outputs": [],
   "source": [
    "# chose one query, and comment out the other\n",
    "#catq = catpm_output.start()\n",
    "wcq = word_counts_output.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a843a1b-47b2-41b0-91a5-07d2e3058ecc",
   "metadata": {},
   "source": [
    "- `query.id()`: get the unique identifier of the running query that persists across restarts from checkpoint data\n",
    "- `query.runId()`: get the unique id of this run of the query, which will be generated at every start/restart\n",
    "- `query.me()`: get the name of the auto-generated or user-specified name\n",
    "- `query.explain()`: print detailed explanations of the query\n",
    "- `query.stop()`: stop the query\n",
    "- `query.awaitTermination()`: block until query is terminated, with stop() or with error\n",
    "- `query.exception()`: the exception if the query has been terminated with error\n",
    "- `query.recentProgress()`: a list of the most recent progress updates for this query\n",
    "- `query.lastProgress()`: the most recent progress update of this streaming query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b49fae-c4f1-4440-ac87-53a205bb2b46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T14:52:50.939562Z",
     "start_time": "2024-01-17T14:52:50.523569700Z"
    }
   },
   "outputs": [],
   "source": [
    "[q.name for q in spark.streams.active]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae5b5d5-c702-4503-b06a-d78b205676aa",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528d1a2-bd2b-49f1-b2d9-9572131855b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T14:54:22.509496700Z",
     "start_time": "2024-01-17T14:54:22.290239Z"
    }
   },
   "outputs": [],
   "source": [
    "# re-execute cell manually\n",
    "spark.sql('SELECT * FROM wordcounts').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaefd95-cda4-4453-afcb-93d14313465f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T14:54:24.895956400Z",
     "start_time": "2024-01-17T14:54:24.448126500Z"
    }
   },
   "outputs": [],
   "source": [
    "# re-execute cell manually\n",
    "spark.sql('SELECT * FROM catpm').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aee09c-d38c-4b55-b264-73812bfa0566",
   "metadata": {},
   "source": [
    "### Automatic Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31269fc2-6f0f-49ec-bc83-9edd5f29e1c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T14:55:09.935389400Z",
     "start_time": "2024-01-17T14:55:09.793999500Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from plotly.graph_objects import FigureWidget\n",
    "\n",
    "initialization_dummy = pd.DataFrame([['a', 0], ['b', 5]], columns=['word', 'count'])\n",
    "f = px.bar(initialization_dummy, x='word', y='count', title=f'The running word counts')\n",
    "fw = FigureWidget(f)\n",
    "\n",
    "def upd_fw_1(df):\n",
    "    df = df.sort_values('count', ascending=False)\n",
    "    w = None\n",
    "    if 0 < len(df): \n",
    "        w = df['window'].iloc[0]\n",
    "    fw.data[0].x = df['word'].values\n",
    "    fw.data[0].y = df['count'].values\n",
    "    fw.update_layout(title_text=f'The running word counts for window {w}')\n",
    "\n",
    "fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194910d98b836fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T14:55:12.365501900Z",
     "start_time": "2024-01-17T14:55:11.703189900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "initialization_dummy = pd.DataFrame([['cat', 0]], columns=['word', 'cats per minute'])\n",
    "f_cat = px.bar(initialization_dummy, y='word', x='cats per minute', range_x=[0, 200], orientation='h', title=f'Occurrences of the word cat per minute')\n",
    "fw_cat = FigureWidget(f_cat)\n",
    "\n",
    "def upd_fw_2(df):\n",
    "    v = 0\n",
    "    if 0 < len(df):\n",
    "        v = df['cats per minute'].iloc[0]\n",
    "    fw_cat.data[0].x = [v]\n",
    "\n",
    "fw_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6d03ceb094e82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T14:54:44.986571500Z",
     "start_time": "2024-01-17T14:54:44.941339100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from asyncio import sleep\n",
    "from IPython.core.display_functions import clear_output\n",
    "\n",
    "def only_latest(df):\n",
    "    return df[df['end'] == df['end'].max()]\n",
    "\n",
    "update_interval = 15 # in seconds\n",
    "\n",
    "async def update_loop(wc_updater, cat_updater):\n",
    "    while True:\n",
    "        clear_output()\n",
    "        df = only_latest(spark.sql('SELECT window.end, * FROM wordcounts').toPandas())\n",
    "        wc_updater(df)\n",
    "        catdf = only_latest(spark.sql('SELECT window.end, * FROM catpm').toPandas())\n",
    "        cat_updater(catdf)\n",
    "        await sleep(update_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc2c8d1cf70bf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T14:54:46.835442600Z",
     "start_time": "2024-01-17T14:54:46.681989500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "task = loop.create_task(update_loop(upd_fw_1, upd_fw_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268fc468-d71b-4d15-83a2-6f3852fa2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f95eaa-a16e-4f43-8d2b-805fdf110f04",
   "metadata": {},
   "source": [
    "### Stopping Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03437fa2ade90b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T14:55:05.650316300Z",
     "start_time": "2024-01-17T14:55:05.619861Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "task.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fdc14f-6d8a-47d6-96f4-9e5f0a1afd25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T12:56:53.647043400Z",
     "start_time": "2024-01-17T12:56:53.582807400Z"
    }
   },
   "outputs": [],
   "source": [
    "[q.stop() for q in spark.streams.active]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9f0f2-1a92-4351-997f-9b6ab42c6540",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T12:56:56.528846300Z",
     "start_time": "2024-01-17T12:56:56.476122Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315662ea-af51-4937-80f0-68bd8e261d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
